---
title: "Collecting Twitter Data With R"
author: "Patricia Rossini, University of Liverpool"
date: "July 2019"
output: html_document

knit: (function(input_file, encoding) {
   rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), 'day1.html'))})


---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
opts_knit$set(root.dir = "~/SM_Methods_SS2019/")
chooseCRANmirror(graphics=FALSE, ind=1)
```


## Introduction 

Welcome to day 1 of the Digital Society International Summer School 2019

Before we can talk about using R to collect and analyze data, let's start with the basics. 
First, why use R when there are so many other options out there -- including some that do not require any coding knowledge? A few reasons: 

- R is open-source, and it is also widely supported by active data science and statistics communities. That means it is free :) and constantly updated with new packages, functions, and utilities 
- R is also one of the most used languages in data science, along with Python. 
- Because of its active community, it's fairly easy to find online resources and help to tackle any problems or challenges you may find
- The use of a command line interface and scripts facilitate reproducibility: not only you can always go back to your script to understand exactly what you did in your analysis, but you can also share it so that others can reproduce it. 
- Stats software can normally only do... statistics. R is a powerful tool that can be used for several different methods (e.g. machine learning, textual analysis, social network analysis, time series), as well as to scrape, clean,  analyze, and visualize data.



### R Basics
First things first: open RStudio and install all the packages we will need for this course running the following code:

```{r installing packages, message=FALSE, eval=FALSE }
install.packages("descr", "rtweet", "dplyr", "ggplot2", "tidyverse", "tidytext", "glue", "stringr", "wordcloud", "lubridate")
```

Now, some basics. 
It is good practice to call your packages in the beginning of your script by using the library command. You may also want to set your work directory and a few options, such as removing scientific notations.


```{r calling packages, eval=FALSE, message=FALSE, inspect=FALSE}
# set work directory
setwd("~/Desktop/SS_Methods_2019/")
# options
options(scipen=999, digits = 4)

```

We will learn the commands we need as we progress, but here are some important things to know: 


```{r basics, eval=FALSE, message=FALSE, include=TRUE}
# assign actions to an object
x <- something

# open csv files
y <- read.csv("path.csv")

#open RData files
load("file.RData")

# save as csv file
write.csv(df, "file.csv", row.names = F)

# save as RData file
save(df, file = "df.RData")
```

Note that there is a code for CSV files and another one for RData files. RData files are great to work in R because you can save multiple objects and open them all with just one command instead of loading a CSV per dataframe. 



## Collecting Twitter Data:

Twitter is among the easiest/most open social media platforms to collect data from. You can get data from Twitter using the API (application programming interface), which require creating a [developers account](http://dev.twitter.com). 

There are several R packages that interact with Twitter's API. We will use rtweet, by [Michael Kearney](https://rtweet.info/articles/intro.html), which can query both the REST and the stream APIs.

Now, you need to authenticate with Twitter using the credentials you created for your app:

```{r twitter oauth, eval=FALSE, include=TRUE}

library(rtweet)

## use your credentials here. If you don't have them, load the ones I shared 
load( "twitter_credentials.RData")


## Create a token
token <- create_token(
  app = app_name, #your app name
  consumer_key = consumerKey, #consumer key for your app
  consumer_secret = consumerSecret) #consumer secret for your app

```

There are different types of Twitter data you can collect. 
For the purposes of this workshop and the hands-on work we do here, we will focus a bit more on the REST API, which collects tweets from a timeline (up to 3200) and from a search (of hashtags, keywords etc), as well as networks of users -- e.g. followers / followed accounts.



```{r search API, eval=FALSE, include=TRUE}

## getting tweets from one user
theresamay <- get_timeline("theresa_may", n=3200, retryOnRateLimit=120, resultType = "recent")


## getting tweets from several users
brits <- get_timelines(c("BorisJohnson", "theresa_may", "jeremycorbyn", "Jeremy_Hunt") , n=3200, retryOnRateLimit=120, resultType = "recent")


save(brits, file = "brits.RData")

``` 

Now we have a dataframe with 90 variables and several observations. Let's take a look at the column names to understand our variables:

```{checking the df}

colnames(brits)
#or
names(brits)

```


With this command, we can see the different columns in our dataframe. We have engagement data (e.g. likes, retweets), data about devices, geolocation etc.

We can also view our dataframe in RStudio by clickong on it in the Environment tab, or inspect the first few columns using: 

```{r inspect, echo=TRUE, eval=FALSE}

head(brits)

```

With speficic accounts, we can also pull their list of followers and followed accounts.

```{r twitter networks, eval=FALSE, include=TRUE, inspect=FALSE}
## how many total follows does the account have?
me <- lookup_users("patyrossini")

## get them all (this would take a little over 5 days because of rate limits)
me_flw <- get_followers(
  "patyrossini", n = me$followers_count, retryonratelimit = TRUE
)

### for accounts FOLLOWED BY me, the command is:

me_friends <- get_friends("patyrossini")

## note that you only get user ids for the accounts that I follow. To get more information, we need to look them up:


me_fds_data <- lookup_users(me_friends$user_id)



```


# Searching terms

Now, a different type of data we can get is a sample of tweets about a topic or hashtag, using search terms and search operators. Bear in mind this is a sample of tweets (capped at 1% of all tweets at the moment of the query). This means that for very popular topics, you are unlikely to get everything. 


```{r search tweets, include=TRUE, eval=FALSE, inspect=FALSE}
brexit <- search_tweets(
  "brexit", n = 5000, include_rts = FALSE, retryonratelimit = TRUE)

```


## Analysis

### Time Series

Time series are useful to understand data patterns over time. For instance, we can plot the four british leaders' Twitter activity in 2019: 

```{r, echo=TRUE}
library(rtweet)

load('brits.RData')

brits %>%
  dplyr::group_by(screen_name) %>% 
  dplyr::filter(created_at >= "2019-01-01 00:00:00") %>% 
  ts_plot("weeks", trim = 2L) +
  geom_abline() +
  theme_classic() +
  scale_x_datetime(date_labels = "%b %d", breaks = "2 week") +
  scale_color_brewer(type = "qual", palette = 2) +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position = "bottom",
    plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "@BorisJohnson, @Theresa_May, @Jeremy_Hunt and @Jeremycorbyn on Twitter, 2019",
    subtitle = "Aggregated by week")  
```

You can tweak your plot changing the colors, breaks (hours, days, weeks...), the type of the plot, the theme etc. Check [ggplot2](https://ggplot2.tidyverse.org/reference/) reference to learn more about these options. 



## Hands-on exercise

```{r eval=FALSE, include=TRUE}
brit_parties <- get_timelines(c("brexitparty_uk", "UKLabour", "Conservatives", "LibDems", "TheGreenParty") , n=3200, retryOnRateLimit=120, resultType = "recent") 


# let's check the frequency for each twitter handle
freq(brit_parties$screen_name)

#and inspect the column names (so we know what we have)
colnames(brit_parties)

#we can filter the results and store them to a new dataframe based on parameters such as the date of the tweeet
parties <- filter(brit_parties, created_at >= "2019-01-01 00:00:00") # gets tweets in 2019
android <- filter(brit_parties, source == "Twitter for Android") #  gets tweets sent using an android phone

# selecting the variables we are interested in and recording them to a new dataframe (useful for large datasets) - needs dplyr to be loaded:

parties_ff <- select(parties, screen_name, favorite_count, retweet_count, reply_count, is_retweet, hashtags, media_type)

# inspecting the dataset to see the values in the subcategories of interest
freq(parties_ff$screen_name)
parties_ff$media_type <- unlist(parties_ff$media_type)
freq(parties_ff$media_type)
freq(parties_ff$hashtags)

```

Basic linear and logistic models:

```{r eval=FALSE, inspect=FALSE, include=TRUE}

## creating binaries for each party, media type and hashtag:
parties_ff$brexitparty <- ifelse(parties_ff$screen_name == "brexitparty_uk", 1, 0)
parties_ff$libdems <- ifelse(parties_ff$screen_name == "LibDems", 1, 0)
parties_ff$greens <- ifelse(parties_ff$screen_name == "TheGreenParty", 1, 0)
parties_ff$labour <- ifelse(parties_ff$screen_name == "UKLabour", 1, 0)
parties_ff$photos <- ifelse(is.na(parties_ff$media_type), 0, 1)
parties_ff$hashtag <- ifelse(is.na(parties_ff$hashtags), 0, 1)

```

Linear model basic code: 
lm(DV ~ IV + IV2, data = dataset), where DV is a numeric variable.

```{r lm, echo=TRUE}
load("~/parties.RData")
library(jtools)
reg1 <- lm(retweet_count ~ photos + hashtag +  labour + greens + libdems + brexitparty, data = parties_ff)
summary(reg1)
# an alternative summary with fit statistics from jtools
summ(reg1)

# a second model predicting favorites: 
reg2 <- lm(favorite_count ~ photos + hashtag +  labour + greens + libdems + brexitparty, data = parties_ff)
summ(reg2)

```

### Saving to a word doc: 

The package jtools has a function to export the summary of your regression model to word. You can set the names of your coefficients, and even combine two models in the same table. Example:

```{r save to word, eval=FALSE, inspect=FALSE, include=TRUE}

# the argument coef_names is only needed if you want R to rename your coefficient names. If not, just remove coef = coef_names in the export_summs function
 
coef_names <- c("Photos" = "photos", "Hashtag" = "hashtag", "Labour Party" = "labour",
                "Greens" = "greens", "LibDems" = "libdems", "Brexit Party" = "brexitparty")

# exporting both models together to a word table
export_summs(reg1, reg2, confint = TRUE, coefs = coef_names,  error_format = '[{conf.low}, {conf.high}]',  ci_level = 0.95,  to.file = "word", model.names = c("Retweets", "Favorites"),  file.name = "Table2.docx")


```


# Logistic Regression (& other general linear models e.g. poisson)

For logistic regression models, predicting binary outcomes, we use a different function: glm(). The function requires you to include family in the call (read the docs to check how). The example below is a logistic model, so we use family = "binomial".

You can use the jtools function above to export the results to a table.

```{r glm, echo = TRUE}

reg3 <- glm(photos ~ greens + brexitparty + libdems + labour, data = parties_ff, family = "binomial") 
summary(reg3)
summ(reg3)

```


Finally, to save your twitter dataset for future analysis (replace the first argument 'brits' with your dataframe): 

```{r saving twitter data, include=TRUE, eval=FALSE, inspect=FALSE}

save(parties_ff, brit_parties, file = "brits.RData")
```

